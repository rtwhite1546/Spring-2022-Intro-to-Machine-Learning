{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4b2896-c99e-452b-8fb6-f1b91b2315c6",
   "metadata": {},
   "source": [
    "# MTH 4224 / CSE 4224 Homework 3\n",
    "\n",
    "## Classification Methods\n",
    "\n",
    "**Deadline**: March 16\n",
    "\n",
    "**Points**: 35\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook for grading. Your file should include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems\n",
    "\n",
    "1. Write a Python class for Fisher's LDA binary classifier in the `scikit-learn` style (i.e. with `fit`, and `predict` functions equipped to accept data matrix inputs). (Details are in *Data Mining and Machine Learning* Ch 20 and *Elements of Statistical Learning* 4.3.3.)\n",
    "\n",
    "2. Write a Python class for discriminant-adapted nearest neighbors (DANN) classifier in the `scikit-learn` style. (Details are in *Elements of Statistical Learning* Ch 13.)\n",
    "\n",
    "For problems 3-5, use the [Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set\n",
    "](https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings) from the UCI Machine Learning Repository.\n",
    "\n",
    "3. Fit each of the classifiers we covered in the course so far to the training data, test on the testing data, and report accuracy metrics under 5-fold cross-validation along with the runtime. Which classifier(s) seem most effective without tuning?\n",
    "\n",
    "(Classifiers: Bayes, naive Bayes, logistic regression, Fisher's LDA, LDA, QDA, k-NN, DANN).\n",
    "\n",
    "4. Use PCA to reduce the dimension of the data as much as possible such that you preserve 90\\% of the total variation in the data. Then apply your two slowest classifiers to the dimension-reduced data. How do the performance and computing time compare?\n",
    "\n",
    "5. Use logistic regression with the LASSO to measure feature importance for this dataset. How many features can be removed without reducing performance more than 10\\%? Which features seem unimportant? Hypothesize why these specific features do not help with classification. (Refer to the real-world things these variables measure.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
