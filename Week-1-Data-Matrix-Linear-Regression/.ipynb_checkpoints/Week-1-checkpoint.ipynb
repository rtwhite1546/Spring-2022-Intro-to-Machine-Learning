{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 1: Machine Learning, Data Matrix, Linear Regression\n",
    "\n",
    "In our first week, we will take a tour of machine learning, review some linear algebra prerequisites (see the class notes), learn how to represent data as numerical matrices, and learn our first ML method: linear regression models.\n",
    "\n",
    "# Lecture 1: Outline of ML, Data Matrices, Linear Regression\n",
    "\n",
    "A machine learning problem generally has three parts\n",
    "\n",
    "* A task $T$\n",
    "* A performance measure $P$\n",
    "* An experience $E$\n",
    "\n",
    "A computer program learns from experience $E$ with respect to some task $T$ and performance measure $P$ if it improves at task $T$, measured by $P$, with experience $E$. For example,\n",
    "\n",
    "* **Task**: Identify pictures as having cats or not having cats in them\n",
    "* **Performance measure**: The proportion of pictures with cats does our computer properly identify\n",
    "* **Experience**: The computer will be given 500 images with cats and 500 images without cats, each with labels of \"cat\" or \"no cat\"\n",
    "\n",
    "Machine learning tasks are generally too difficult for pre-designed programs to do. Instead, we write programs for computers to learn to solve them. For example, we might want a machine learning algorithm to determine if a picture has a cat in it:\n",
    "\n",
    "* We **WOULD NOT** give a computer set of instructions to decide how to find cats. (What would the instructions be?!)\n",
    "\n",
    "* We **WOULD** feed many cat pictures and non-cat pictures to the computer, let label them according to some instructions (usually with some randomly initialized parameters), and give it some instructions for how to tweak its labels, and \n",
    "\n",
    "There are many types of tasks. Two large classes of tasks are supervised and unsupervised learning tasks.\n",
    "\n",
    "* **Supervised learning algorithms** have an experience of observing a dataset of examples *with* labels a correct algorithm would output.\n",
    "\n",
    "* **Unsupervised learning algorithms** have an experience of observing a dataset *without* labels and seek to learn useful patterns in the dataset.\n",
    "    \n",
    "There are some other types of problems, but these groups are most common and the primary focus of this class. Below, we discuss some common tasks in each category. It is not meant to be an exhaustive list but rather a brief outline of what the tasks are, examples of each, and some common methods for attacking these tasks.\n",
    "\n",
    "It should be noted that machine learning practitioners commonly use multiple methods, build methods customized to their problems, and create pipelines using multiple methods in a specified sequence.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "For most supervised learning problems, we have a dataset of $n$ **examples** or **data points**, which can be presented as points in space\n",
    "\n",
    "$$ x_i=(x_{i1}, ..., x_{id})\\in\\mathbb{R}^d$$\n",
    "\n",
    "and we try to predict a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$ mapping these points $x_i$ to some **label** or **target** $y_i\\in\\mathbb{R}^m$. Each component of $x_i$ is usually called a **feature** or an **attribute**.\n",
    "\n",
    "### Regression Problems\n",
    "\n",
    "In a regression task, the learning algorithm tries to predict a numerical $m$-vector output given an $d$-dimensional input example--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$.\n",
    "\n",
    "An example is predicting the price for which a house will sell given information on the house--the number of bedrooms, the number of bathrooms, the floorspace, the size of the surrounding yard, whether or not it has a pool, etc. Here, as in many of the problems we will cover, $m=1$, meaning we will predict only one output variable.\n",
    "\n",
    "Common methods for regression include linear regression, ridge and LASSO regression, decision trees, random forests, and support vector machines--all of which will be covered in this course--as well as neural networks, which are beyond the scope of this course.\n",
    "\n",
    "### Classification Problems\n",
    "\n",
    "In a classification task, the learning algorithm tries to predict which of $k$ disjoint categories a datapoint belongs to--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\{c_1, ..., c_k\\}$.\n",
    "\n",
    "Identifying pictures of cats is an example of this problem: the categories are \"cat\" and \"not cat,\" which are mutually exclusive. (Keep in mind an image is stored in a computer as a numerical value representing the intensity of red, blue, and green colors in each pixel of the image, which may easily be stored as a very high-dimensional point.)\n",
    "\n",
    "Common methods for classification include logistic regresion, $k$-nearest neighbors, the Bayes and naive Bayes classifiers, discriminant analysis (LDA and QDA), decision trees, random forests, and support vector machines--all of which will be covered in this course--as well as neural networks, which are beyond the scope of this course.\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, we do not have the benefit of knowing some of the results we need to find. It is usually a somewhat less structured search for useful patterns in a dataset.\n",
    "\n",
    "### Clustering Problems\n",
    "\n",
    "A clustering task is one that tries to find which datapoints are similar to one another, in some sense that we do not necessarily define in advance.\n",
    "\n",
    "Common methods for clustering are K-means, hierarchical clustering, Gaussian mixtures, and density-based clustering--which we will cover in the course--as well as self-organizing maps, which are beyond the scope of this course.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "In a dimensionality reduction task, the goal is to represent a dataset using fewer features without obscuring useful patterns in the data.\n",
    "\n",
    "For example, if we have a 1-megapixel color picture as a datapoint, it would have 1 million pixels and we would store three numbers (R, G, and B values) for each pixel, meaning the dimension of the image would be 3 million. It is sometimes far too slow to use datapoints in $\\mathbb{R}^{3000000}$ in machine learning algorithms. It is frequently helpful to find ways to store datapoints in lower-dimensional spaces such that important information is not lost. The idea is similar to compression.\n",
    "\n",
    "Common methods for dimensionality reduction are principal components analysis (PCA), multidimensional scaling (MDS), and $t$-stochastic neighbors embedding ($t$-SNE)--which we will cover in the course--as well as autoencoders and word embeddings, which are beyond the scope of this course.\n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "In an anomaly detection task, the goal is to find unusual patterns in data.\n",
    "\n",
    "An example is credit card companies trying to detect unusual usage of a credit card. If they can detect unusual activity, they sometimes deactive credit cards to avoid fraud. False positives may cause problems for legitimate customers, but cards deactivated due to actual fraud can prevent further damage. \n",
    "\n",
    "### Denoising\n",
    "\n",
    "In denoising, the goal is to uncover some original dataset that has been corrupted by some sort of noise, whether we mean noisy sounds or random error. In all cases, the goal is to find a sometimes-faint signal within some noise.\n",
    "\n",
    "Denoising is not a task we will cover explicitly in the course.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* If you are speaking into a phone while riding in a car, the road noise can cause the voice signal not to be transmitted clearly\n",
    "* Noise-cancelling headphones try to counteract noise to pass through clear audio\n",
    "* Random errors in measurements in particle physics make the signal difficult to extract\n",
    "* Financial instruments like stocks can fluctuate at random while there is an underlying cause of general trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix\n",
    "\n",
    "In this section, we will cover how data is frequently stored such that it can be used by machine learning methods. It should not be thought that this is the only way or always the best way to store data, but it will be a series of conventions used in many fields.\n",
    "\n",
    "For many problems, we will have a **dataset** consisting of some number $n$ points in $\\mathbb{R}^d$ that we will store in a matrix\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The $i$th *row* $x_i = (x_{i1}, x_{i2}, ..., x_{id})\\in\\mathbb{R}^d$ is the $i$th point in the dataset. These points have many names in different fields: **points, datapoints, examples, vectors, records, feature-vectors**. In some sources, these points $x_i$ are denoted $\\mathbf{x}_i$, but it should be clear that $x$ with a single subscripts indicates a point while $x$ with two subscripts is the component of a point.\n",
    "\n",
    "The $j$th *column* $X_j=(x_{1j}, x_{2j}, ..., x_{nj})\\in\\mathbb{R}^n$ is the $j$th component of each point in the dataset. These are likewise called by many names dependent on field: **features, attributes, variables, dimensions, properties, fields**. In some cases, each column can be considered a random sample of a random variable, or the rows of the matrix $X$ can be considered a random sample of vector-valued random variables.\n",
    "\n",
    "The number of points $n$ is the **size** of the dataset and the length of the points $d$ is called the **dimensionality** of the dataset.\n",
    "\n",
    "### Data Representation Examples\n",
    "\n",
    "\n",
    "* If we have a dataset of sounds of people speaking along with transcripts of the words, we might want to classify the words spoken into a microphone. (Think Siri!)\n",
    "\n",
    "In all of these cases, the information can be represented as a point in the $n$-dimensional real space.\n",
    "\n",
    "* Suppose we have a dataset of medical records where each datapoint contains details for one patient in the image below. Here, a datapoint for a single patient might be stored in the 9D space $\\mathbb{R}^9$ as below, where each component with a question mark is stored as a binary digit.\n",
    "\n",
    "$$(male?, female?, age, weight, height, smoker?, white?, black?, asian?)$$\n",
    "\n",
    "<center><img src=\"images/medicalRecords.png\" /></center>\n",
    "\n",
    "* Suppose we have a dataset of labeled color images of cat and dogs. Images are typically represented by three channels, meaning three numbers for each pixel (the red, green, and blue levels), like the bird picture below. Each channel is stored as matrix of intensities, as in the picture of the dog and cat below. Although it may seem awkward, we can reshape this data into a giant vector.\n",
    "\n",
    "<center><img src=\"images/dogcat.png\" /></center>\n",
    "\n",
    "<center><img src=\"images/birdRGB.png\" /></center>\n",
    "\n",
    "* If we have a dataset of audio files, each of which is a clip of a jazz, classical, rock, pop, or hip-hop song, the audio files might have several numbers describing the individual sounds for the song sampled many times per second. This, too, can be reshaped into a vector.\n",
    "\n",
    "<center><img src=\"images/audioFile.png\" /></center>\n",
    "\n",
    "* The audio files might have numbers specifying the type of sound for a word many times per second. The blue lines indicate the beginning of a word and the red lines indicate the ends of words.\n",
    "\n",
    "<center><img src=\"images/audioWords.png\" /></center>\n",
    "\n",
    "* If we have a dataset of traffic logs on a network, some known to be infected by a specific virus and some are not, we might store this information to try to classify which logs are likely infected. The network traffic logs might have numbers of packets transferred, file size, ports, addresses, the content of the packets, etc., at each moment in time, which may be reshaped into a vector.\n",
    "\n",
    "\n",
    "In all mature applications, we should expect preprocessing steps to be done to raw data before ML can be effectively used to solve supervised or unsupervised problems.\n",
    "\n",
    "I chose these applications to demonstrate two things: (1) ML problems are interesting and useful in almost every area of study and (2) a huge class of problems have much in common mathematically. All apply to datapoints, although some types of data may have far more dimensions than others--a medical record may only have 9 numbers, but a 12-megapixel photo from the latest iPhone would have $4,000\\times 3,000\\times 3=36,000,000$ numbers, 3 for each pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Regression problems** are problems where we try to predict a numerical output value given some input datapoint based on some examples ($x_i$, $y_i$) of input datapoints with known outputs or **targets**. This will be the task of the machine learning problem.\n",
    "\n",
    "### Examples\n",
    "\n",
    "1. If we have a dataset of workers in a certain position of numbers of years of experience as an input and salary, we might want to predict salaries we don't know based on the years of experience a person has.\n",
    "\n",
    "1. If we have a dataset of variables about houses (floorspace, number of bedrooms, number of bathrooms, number of stories, age of the house) along with their selling prices, we might want to predict selling prices of homes not in the dataset based on the other variables about the house.\n",
    "\n",
    "1. If we have a dataset of variables about countries (average salary, average education of citizens, death rate, birth rate, infant mortality rate, etc.) along with their GDP, we might want to predict the GDP of countires not in the dataset based on the other variables about the country.\n",
    "\n",
    "1. If we have a dataset of seasonal variables about NBA basketball teams (points per game, turnovers per game, point differential, rebounds per game, blocks per game, etc.) and the numbers of wins they had in different seasons, we might want to take the statistics of a team early in a season to try to predict the number of wins they will have and average number of points per game.\n",
    "\n",
    "### Types of Regression Problems\n",
    "\n",
    "There are different types of regression problems based on the numbers of input variables and output variables.\n",
    "\n",
    "* A **simple** regression problem predicts an output variable with just one input variable, as in example 1.\n",
    "\n",
    "* A **multiple** regression problem predicts an output variable with more than one input variable as in examples 2 and 3.\n",
    "\n",
    "* A **multivariate** regression problem predicts more than one output variables as in example 4.\n",
    "\n",
    "### The Math of a Regression Problem\n",
    "\n",
    "All of these regression problems have some things in common: there are example datapoints with outputs and we want to predict the outputs for new datapoints. Consider a $d$-dimensional point, or vector, $x_i\\in\\mathbb{R}^d$ and denote $x_i=(x_{i1},x_{i2},...,x_{id})$. $x_i$ maps to an output $y_i\\in\\mathbb{R}^m$. (In statistics, the components of the vector $x_1$ are more frequently called **predictors** or **independent variables** and the $y_i$ values are more frequently called **responses** or **dependent variables**.)\n",
    "\n",
    "In a perfect world, a solution to a (univariate) regression problem will find a function $\\hat{f}:\\mathbb{R}^d\\to\\mathbb{R}$ that can do two things:\n",
    "\n",
    "1. Mapping each example $x_i$ in a dataset to its output $y_i=\\hat{f}(x_i)$\n",
    "1. Generalize to successfully predict outputs of new datapoints\n",
    "\n",
    "In reality, $\\hat{f}$ will not always map each input $x_i$ values to each $y_i$ value perfectly or generalize to new inputs well, but we try to get as close to these ideals as possible.\n",
    "\n",
    "### Regression Algorithms\n",
    "\n",
    "There are a number of popular approaches to regression problems, including the following.\n",
    "\n",
    "* linear regression\n",
    "* structured regression\n",
    "* radial basis function networks\n",
    "* ridge and LASSO regression\n",
    "* decision trees\n",
    "* random forests\n",
    "* support vector machines\n",
    "* neural networks\n",
    "\n",
    "In the near term, we will consider the first four approaches as they work in relatively similar ways. As an added bonus, they all *can* use a numerical optimization scheme called gradient descent, which is one of the most highly-used algorithms of machine learning.\n",
    "\n",
    "**Note**: some of the methods can be easily solved explicitly rather than resorting to numerical optimization.\n",
    "\n",
    "Later in the term, we will focus on trees, forests, and SVMs, but neural networks are outside the scope of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "We will assume the function $\\hat{f}$ we will predict is linear in some parameters $\\theta_0,...,\\theta_d$ so that our predicted function will be\n",
    "\n",
    "$$\n",
    "\\hat{f}(x_i)=\\theta_0 + \\sum\\limits_{k=1}^d \\theta_k x_{ik}=\\theta_0+\\theta_1 x_{i1}+\\cdots+\\theta_d x_{id}=(1, x_{i1}, \\dots, x_{id})\\cdot(\\theta_0, \\theta_1, \\dots, \\theta_d)\n",
    "$$\n",
    "\n",
    "for some parameters $\\theta_0,...,\\theta_d$. More succinctly, we will write\n",
    "\n",
    "$$\n",
    "\\hat{f}(x_i)=x_i^T\\theta\n",
    "$$\n",
    "\n",
    "Here, we assume the output $Y$ is a random variable of the form\n",
    "\n",
    "$$Y = \\hat{f}(X_1, ..., X_d) + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon$ is a random error term that is assumed to account for the effect of unknown latent variables and intrinsic randomness of $y$.\n",
    "\n",
    "We will aim to choose the values of $\\theta_0,...,\\theta_d$ that will minimize a loss function on a training dataset $(x_1,y_1),...,(x_n,y_n)$. This loss function will be small if each $\\hat{f}(x_i)$ is near each $y_i$, which is what we want.\n",
    "\n",
    "#### Note\n",
    "\n",
    "It is a common misconception that \"linear regression\" must fit linear functions with respect to data. However, but the \"linear\" part of linear regression refers to the fact that $f$ is linear with respect to $\\theta_0,...,\\theta_d$, not with respect to $x_i$, so it is certainly possible to apply some preprocessing to the datapoints, which in effect, fits a nonlinear surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Recall that we have a labeled dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with each $x_i\\in\\mathbb{R}^d$ and each $y_i\\in\\mathbb{R}$ and our goal is to find a function $\\hat{f}$ that maps the $x_i$'s to the $y_i$'s as well as possible, and, we hope, is effective at mapping unknown datapoints to appropriate outputs.\n",
    "\n",
    "In the **ordinary least squares** method, we try to minimize a the **sum of squared differences** between the real outputs $y_1, ..., y_n$ and the predicted outputs $\\hat{f}(x_1)$, ..., $\\hat{f}(x_n)$. In other words, the loss function in this method is\n",
    "\n",
    "$$L(\\theta)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 = \\sum\\limits_{i=1}^n \\left(x_i^T\\theta-y_i\\right)^2=\\|X\\theta-y\\|^2$$\n",
    "\n",
    "which measures the error, where\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\\theta=\\begin{pmatrix}\n",
    "\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is the data matrix, except we added a column of ones to the left for convenience.\n",
    "\n",
    "The ordinary least squares problem aims to solve the optimization problem\n",
    "\n",
    "$$\n",
    "\\min\\limits_{\\theta\\in\\mathbb{R}^{d+1}} L(\\theta)=\\min\\limits_{\\theta\\in\\mathbb{R}^{d+1}}\\|X\\theta-y\\|^2\n",
    "$$\n",
    "\n",
    "Here, $X$ and $y$ are constants derived from the dataset, so the $\\theta$ values are the only unknowns, so we need to find the $\\theta$ values that minimize the loss function $L$. We should note that these $\\theta$ values are called **parameters** of the model, which are values that are estimated by the model automatically from the data.\n",
    "\n",
    "It turns out, we can actually solve this optimization explicitly under a minor assumption that the columns of the data are linearly independent, so let's solve it. In multivariate calculus, the approach to minimize a differentiable function with unbounded domain is to take derivatives with respect to $\\theta_0, ..., \\theta_d$, set them all equal to zero, solve simultaneously, and compare the function at these critical values. The OLS problem has a mathematical property called convexity, meaning it has a unique solution, which will occur at the critical point.\n",
    "\n",
    "Before we take derivatives, let's convert the loss function into a more convenient form. First,\n",
    "\n",
    "$$L(\\theta)=\\|X\\theta-y\\|^2_2=(X\\theta-y)^T(X\\theta-y),$$\n",
    "\n",
    "where the $T$ subscript represents the **transpose** of a matrix. Distributing the transpose,\n",
    "\n",
    "$$L(\\theta)=((X\\theta)^T-y^T)(X\\theta-y).$$\n",
    "\n",
    "Using the distributive property of matrix multiplication,\n",
    "\n",
    "$$L(\\theta)=(X\\theta)^T X\\theta-(X\\theta)^T y-y^T X\\theta+y^T y$$\n",
    "\n",
    "If we realize $y$ and $X\\theta$ are both matrices of shape $n\\times 1$, then we should have $(X\\theta)^T y=y^T X\\theta$, so the loss function is\n",
    "\n",
    "$$L(\\theta)=(X\\theta)^T X\\theta-2(X\\theta)^T y+y^T y$$\n",
    "\n",
    "Since $(AB)^T=B^TA^T$ for matrices, we can simplify the terms as\n",
    "\n",
    "$$L(\\theta)=\\theta^T X^T X\\theta-2\\theta^T X^T y+y^T y$$\n",
    "\n",
    "Now, of course, this is a scalar (because, in the end, the loss is just a number--the sum of squared errors), so we can take the derivatives with respect to $\\theta_1$, ..., $\\theta_d$ and create the gradient vector:\n",
    "\n",
    "$$\\nabla L(\\theta)=2X^T X\\theta-2X^T y$$\n",
    "\n",
    "As in multivariate calculus, we set this whole vector equal to 0 and solve for the estimated version of $\\theta$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2X^T X\\theta-2X^T y &= 0\\\\\n",
    "X^T X\\theta &= X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, if $X^T X$ is an invertible matrix, then we can left-multiply both sides of the equation by its inverse to solve for $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X^T X)^{-1}(X^T X\\theta) &= (X^T X)^{-1}X^T y\\\\\n",
    "\\theta &=(X^T X)^{-1}X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "At long last, we have a formula for the exact solution for the $\\theta$ values that minimize the loss function.\n",
    "\n",
    "#### Linear Algebra Notes\n",
    "\n",
    "* The formula above holds only if the inverse of the matrix $X^T X$ exists. Assuming $n\\geq d$, the inverse exists when the columns of $X$ are linearly independent. (See <a href=\"https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/lin-alg-showing-that-a-transpose-x-a-is-invertible\">this video</a> or pretty much any linear algebra book.)\n",
    "\n",
    "* If you have studied linear algebra, you might recognize $(X^T X)^{-1}X^T$ is the <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">Moore-Penrose pseudoinverse</a> of $X$.\n",
    "\n",
    "Anyway, the formula for $\\theta$ does not look so nice to do by hand since it requires a matrix multiplication, a matrix inverse, and two more matrix multiplications, but a computer can complete these tasks quickly. (The <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">best algorithms</a> for matrix multiplication and matrix inverses for $n\\times n$ matrices each have computational complexity less than $O(n^{2.5})$, so doing a few of these is no problem, even for quite large matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Code\n",
    "\n",
    "Before we write some code for ordinary least squares, let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class for using ordinary least squares in this way to fit the model and to predict outputs for unknown inputs. We will use the scikit-learn pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquares:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the outputs\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing a Regression Model\n",
    "\n",
    "If we have a dataset of labeled data, a very common approach is to randomly split the dataset into two parts: the training set and the testing set. We remove the labels from the test set, \"train\" the model with the training set, use the resulting model to attempt to make predictions for the test set, and then measure performance.\n",
    "\n",
    "There are no strict rules here, but it is common to use 60\\% train 40\\% test, although folks like Andrew Ng have argued that a much smaller test sets are reasonable if the dataset is very large--e.g. datasets with millions of datapoints are not uncommon in some fields. This way, we can measure the success of our regression model on data it has never seen (the testing set). Once we become confident our model works well in this way, we can be more confident that it will **generalize** well in the real world.\n",
    "\n",
    "My preferred approach is to use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a> function from the <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\">scikit-learn.model_selection</a> library to randomly assign a specified percentage (usually 50-75%) of the dataset to the training set and the rest in the test set.\n",
    "\n",
    "### Hyperparameters and Dev Sets\n",
    "\n",
    "In some models, there are **hyperparameters** to tune. These are settings the user specifies before running the algorithms which may have an impact on performance. In this case, the test set is frequently split in half into \"dev\" and \"test\" sets. The dev set is used for tuning the hyperparameters before testing the model on unknown test sets. More on that later.\n",
    "\n",
    "## Performance Metrics for Regression\n",
    "\n",
    "Let's consider a few performance metrics in common usage for regression.\n",
    "\n",
    "With linear regression, we generally have the unfortunate situation that the model we construct is not perfect even on the training set. Therefore, we first need to consider the performance on the training data to which it was fit just to see how well the model fits to the training data. The formulas for the common metrics are not particularly interesting, so we just state what they represent and what value they should ideally have:\n",
    "\n",
    "* **Coefficient of determination** $R^2$ - the fraction of the variation in the data explained by the model. It is, in a sense, a measure of the strength of the linear relationship between the variables. Ideally, it will be near 1.\n",
    "\n",
    "* **Sum of squared error (SSE)** - the loss function we have used. Ideally, it will be as small as possible.\n",
    "\n",
    "* **Mean squared error (MSE)** is simply the SSE divided by the number of examples we test. It is frequently better because it makes little sense to measure success in a way that is so depedent on the dataset size. Ideally, it should of course be small.\n",
    "\n",
    "* **Mean absolute error (MAE)** - the mean of the absolute errors between the points and the fitted function. Ideally, it will be as small as possible.\n",
    "\n",
    "If these values are far from their ideal values for the training set, the model does not even fit the training data well, so it probably will not fit the testing data well. The ordinary least squares solution finds the optimal parameters for a linear fit, so poor performance on the training set means the data do not have a strong linear relationship.\n",
    "\n",
    "Some preprocessing of the data might make it work better. For example, you can apply a logarithm to a variable if there's a linear relationship with that variable on a log scale. See the (free) classic book <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">*Elements of Statistical Learning*</a> by Hastie, et. al., section 2.6.3 for an introduction on linear basis expansions.\n",
    "\n",
    "Second, we need to consider the performance on the testing data. We generally should consider the MSE or MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Example\n",
    "\n",
    "Let's make up some 1D data and test the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted y values are [1.2 1.9 2.6 3.3 4. ]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The beta values are [-3.   0.7]\n",
      "The r^2 score is 0.9423076923076923\n",
      "The mean squared error is 0.060000000000000074\n",
      "The mean absolute error is 0.20000000000000212\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = OrdinaryLeastSquares()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions)\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = model.theta\n",
    "print('The beta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Example\n",
    "\n",
    "Let's make up some 2D data and test to see if our method works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.9642679900744417\n",
      "The mean squared error on the training set is 4.607999999999986\n",
      "The mean absolute error on the training set is 1.5360000000002088\n",
      "The predicted y values for the test set are -6.520000000000415\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The beta values are [-3.56 -6.24  9.52]\n",
      "The mean squared error on the test set is 149.37920000000292\n",
      "The mean absolute error on the test set is 9.80000000000029\n"
     ]
    }
   ],
   "source": [
    "trainX = np.array([[2, 2], [2, 3], [5, 6], [6, 7], [9, 10]])\n",
    "trainY = np.array([3, 13, 19, 29, 35])\n",
    "\n",
    "testX = np.array([[2, 1], [4, 5], [6, 5], [8, 9]])\n",
    "testY = np.array([9, 15, 25, 31])\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquares()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.theta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a low dimensional problem like this one, we can plot the points along with the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'y')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "# scatter plot of the test data\n",
    "x1 = testX[:,0]\n",
    "x2 = testX[:,1]\n",
    "y = testY\n",
    "ax.scatter(x1, x2, y, marker = 'o')\n",
    "\n",
    "# scatter plot of the training data\n",
    "x1 = trainX[:,0]\n",
    "x2 = trainX[:,1]\n",
    "y = trainY\n",
    "ax.scatter(x1, x2, y, marker = '^')\n",
    "\n",
    "# plot the plane we fit to the data\n",
    "theta = model.theta\n",
    "\n",
    "# surface plot\n",
    "x1 = np.linspace(0,10,10)\n",
    "x2 = np.linspace(0,10,10)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "Y = theta[0] + theta[1]*X1 + theta[2]*X2\n",
    "\n",
    "surf = ax.plot_wireframe(X1, X2, Y)\n",
    "ax.view_init(20, 20)\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: High School Graduation Rates in US States\n",
    "\n",
    "Let's try to use ordinary least squares on a real dataset. The CSV file in '/data/US_State_data.csv' contains data from each U.S. state.\n",
    "\n",
    "We would like to predict the output variable included, the high school graduation rate, from some input variables: including the crime rate (per 100,000 persons), the violent crime rate (per 100,000 persons), average teacher salary, student-to-teacher ratio, education expenditure per student, population density, and median household income.\n",
    "\n",
    "This means we have 50 examples (one for each state), 7 input (predictor) variables, and one output (response) variable. In order to use the formula we derived above to attack the problem with ordinary least squares, we need to find the matrices $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.40352482633726694\n",
      "The mean squared error on the training set is 19.231858411594622\n",
      "The mean absolute error on the training set is 3.66714266824912\n",
      "The predicted y values for the test set are 81.0\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The beta values are [ 1.13013011e+02 -6.03346210e-03  6.08210993e-03 -2.31029129e-04\n",
      " -2.66944638e-01 -4.46787015e-04  1.04141591e-02  8.22209664e-05]\n",
      "The mean squared error on the test set is 26.74145509813265\n",
      "The mean absolute error on the test set is 4.380721146015775\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pd.read_csv('../data/US_State_Data.csv', sep=',').to_numpy()\n",
    "\n",
    "# select the data and the labels\n",
    "X = np.array(data[:,1:8], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "# run the model (same code as above)\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquares()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.theta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Air Pollution\n",
    "\n",
    "We will next work on a somewhat extensive example with a real dataset. It will be a pretty realistic problem because the data is not quite pristine and will require some preprocessing before it can be modeled well.\n",
    "\n",
    "To facilitate that, we import some libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the Beijing Multi-Site Air-Quality Data Data Set dataset$^1$ available from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data). It is a hourly data set considers 6 main air pollutants and 6 relevant meteorological variables at multiple sites in Beijing.\n",
    "\n",
    "[1] Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. *Proceedings of the Royal Society A*, Volume 473, No. 2205. https://doi.org/10.1098/rspa.2017.0457\n",
    "\n",
    "The dataset includes a number of variables associated with time, weather, and location: the year, month, day, hour, temperature, pressure, dew point, precipitation, wind direction, wind speed, and station name.\n",
    "\n",
    "The dataset also includes air pollutant levels for: fine inhalable particulate matter with diameter $\\leq 2.5\\,\\mu$m (PM$_{2.5}$), inhalable particles with diameter $\\leq 10\\, \\mu$m (PM$_{10}$), sulfur dioxide (SO$_2$), nitrogen dioxide (NO$_2$), carbon monoxide (CO), and ozone (O$_3$).\n",
    "\n",
    "We will try to predict these pollution levels based the time, weather, and location variables.\n",
    "\n",
    "This dataset is not as neat as the US high school graduation data: there are numerical variables, but there are a few new wrinkles:\n",
    "\n",
    "* Some variables are text\n",
    "* There is missing numbers in the dataset\n",
    "* There is a variable that simply indexes the data\n",
    "\n",
    "All of these issues would prevent us from using ordinary least squares, so we need to solve these issues. In most applied problems, there are similar issues that must be managed before you can do much machine learning.\n",
    "\n",
    "#### Cleaning/Preprocessing the Data\n",
    "\n",
    "First, we must **clean** the data (manipulate it into a data matrix we can use for machine learning). The data is stored in 12 separate comma-separated value (CSV) files, so we will use the `glob` library to iterate through the files and use the `pandas` library to store each as a dataframe and then concatenate them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>35060</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>35061</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>35062</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>35063</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>35064</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420768 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          No  year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  \\\n",
       "0          1  2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0   \n",
       "1          2  2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0   \n",
       "2          3  2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0   \n",
       "3          4  2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0   \n",
       "4          5  2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0   \n",
       "...      ...   ...    ...  ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "35059  35060  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0   \n",
       "35060  35061  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0   \n",
       "35061  35062  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0   \n",
       "35062  35063  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0   \n",
       "35063  35064  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0   \n",
       "\n",
       "       TEMP    PRES  DEWP  RAIN   wd  WSPM        station  \n",
       "0      -0.7  1023.0 -18.8   0.0  NNW   4.4   Aotizhongxin  \n",
       "1      -1.1  1023.2 -18.2   0.0    N   4.7   Aotizhongxin  \n",
       "2      -1.1  1023.5 -18.2   0.0  NNW   5.6   Aotizhongxin  \n",
       "3      -1.4  1024.5 -19.4   0.0   NW   3.1   Aotizhongxin  \n",
       "4      -2.0  1025.2 -19.5   0.0    N   2.0   Aotizhongxin  \n",
       "...     ...     ...   ...   ...  ...   ...            ...  \n",
       "35059  12.5  1013.5 -16.2   0.0   NW   2.4  Wanshouxigong  \n",
       "35060  11.6  1013.6 -15.1   0.0  WNW   0.9  Wanshouxigong  \n",
       "35061  10.8  1014.2 -13.3   0.0   NW   1.1  Wanshouxigong  \n",
       "35062  10.5  1014.4 -12.9   0.0  NNW   1.2  Wanshouxigong  \n",
       "35063   8.6  1014.1 -15.9   0.0  NNE   1.3  Wanshouxigong  \n",
       "\n",
       "[420768 rows x 18 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# import all the files in data/PRSA\n",
    "dataFiles = glob.glob('../data/PRSA/*')\n",
    "\n",
    "# empty list to store the data\n",
    "data = []\n",
    "\n",
    "# iterate through files, read files\n",
    "for file in dataFiles:\n",
    "    # convert file to dataframe and add it to the list\n",
    "    data.append(pd.read_csv(file, sep = ','))\n",
    "    \n",
    "# concatenate all the dataframes into one\n",
    "data = pd.concat(data)\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use pandas to drop unimportant variables:\n",
    "\n",
    "* Drop the '**No**' column with `drop()` since it just stores an index that has no physical significance.\n",
    "* Drop rows with empty values with `dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  TEMP  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  -0.7   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  -1.1   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  -1.1   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  -1.4   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  -2.0   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...   ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  12.5   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  11.6   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  10.8   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  10.5   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0   8.6   \n",
       "\n",
       "         PRES  DEWP  RAIN   wd  WSPM        station  \n",
       "0      1023.0 -18.8   0.0  NNW   4.4   Aotizhongxin  \n",
       "1      1023.2 -18.2   0.0    N   4.7   Aotizhongxin  \n",
       "2      1023.5 -18.2   0.0  NNW   5.6   Aotizhongxin  \n",
       "3      1024.5 -19.4   0.0   NW   3.1   Aotizhongxin  \n",
       "4      1025.2 -19.5   0.0    N   2.0   Aotizhongxin  \n",
       "...       ...   ...   ...  ...   ...            ...  \n",
       "35059  1013.5 -16.2   0.0   NW   2.4  Wanshouxigong  \n",
       "35060  1013.6 -15.1   0.0  WNW   0.9  Wanshouxigong  \n",
       "35061  1014.2 -13.3   0.0   NW   1.1  Wanshouxigong  \n",
       "35062  1014.4 -12.9   0.0  NNW   1.2  Wanshouxigong  \n",
       "35063  1014.1 -15.9   0.0  NNE   1.3  Wanshouxigong  \n",
       "\n",
       "[382168 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the 'No' column\n",
    "data = data.drop(columns = ['No'])\n",
    "\n",
    "# drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting a Categorical Variable to One-Hot\n",
    "\n",
    "Note the number of datapoints went from 420768 to 382168, a loss of about 9\\% of the data, due to some data being incomplete. Simply dropping datapoints can sometimes bias the model, but this is a relatively small amount of data, so it should not be a big problem.\n",
    "\n",
    "The next problem we have is that the **station** variable is not numerical but is rather categorical, representing the site where the datapoint was measured. This does not work with ordinary least squares. We could delete them from the dataset as well, but these sites are in different parts of the city which may experience different pollution patterns, so this information may help the model make predictions.\n",
    "\n",
    "One way to deal with categorical variables in machine learning problems is to convert them to **one-hot vectors** which are standard basis vectors of $\\mathbb{R}^m$ where $m$ is the number of categories. In other words, they are made up of all 0s except for one 1, representing the one category in the datapoint.\n",
    "\n",
    "Luckily, `Pandas` has a `get_dummies()` function, which does this conversion for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>...</th>\n",
       "      <th>station_Dingling</th>\n",
       "      <th>station_Dongsi</th>\n",
       "      <th>station_Guanyuan</th>\n",
       "      <th>station_Gucheng</th>\n",
       "      <th>station_Huairou</th>\n",
       "      <th>station_Nongzhanguan</th>\n",
       "      <th>station_Shunyi</th>\n",
       "      <th>station_Tiantan</th>\n",
       "      <th>station_Wanliu</th>\n",
       "      <th>station_Wanshouxigong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  ...  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  ...   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  ...   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  ...   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  ...   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  ...   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...  ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  ...   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  ...   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  ...   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  ...   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0  ...   \n",
       "\n",
       "       station_Dingling  station_Dongsi  station_Guanyuan  station_Gucheng  \\\n",
       "0                     0               0                 0                0   \n",
       "1                     0               0                 0                0   \n",
       "2                     0               0                 0                0   \n",
       "3                     0               0                 0                0   \n",
       "4                     0               0                 0                0   \n",
       "...                 ...             ...               ...              ...   \n",
       "35059                 0               0                 0                0   \n",
       "35060                 0               0                 0                0   \n",
       "35061                 0               0                 0                0   \n",
       "35062                 0               0                 0                0   \n",
       "35063                 0               0                 0                0   \n",
       "\n",
       "      station_Huairou  station_Nongzhanguan  station_Shunyi  station_Tiantan  \\\n",
       "0                   0                     0               0                0   \n",
       "1                   0                     0               0                0   \n",
       "2                   0                     0               0                0   \n",
       "3                   0                     0               0                0   \n",
       "4                   0                     0               0                0   \n",
       "...               ...                   ...             ...              ...   \n",
       "35059               0                     0               0                0   \n",
       "35060               0                     0               0                0   \n",
       "35061               0                     0               0                0   \n",
       "35062               0                     0               0                0   \n",
       "35063               0                     0               0                0   \n",
       "\n",
       "       station_Wanliu  station_Wanshouxigong  \n",
       "0                   0                      0  \n",
       "1                   0                      0  \n",
       "2                   0                      0  \n",
       "3                   0                      0  \n",
       "4                   0                      0  \n",
       "...               ...                    ...  \n",
       "35059               0                      1  \n",
       "35060               0                      1  \n",
       "35061               0                      1  \n",
       "35062               0                      1  \n",
       "35063               0                      1  \n",
       "\n",
       "[382168 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the 'station' column to binary variables\n",
    "data = pd.get_dummies(data, columns = ['station'])\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the station variable has been replaced with a binary variable for each station. In all, we lost 1 column (**station**) and gained 12 more, **station_(station name)**, resulting in 28 total columns.\n",
    "\n",
    "#### Wind Direction\n",
    "\n",
    "The last problem we see is the wind direction column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NNW\n",
       "1          N\n",
       "2        NNW\n",
       "3         NW\n",
       "4          N\n",
       "        ... \n",
       "35059     NW\n",
       "35060    WNW\n",
       "35061     NW\n",
       "35062    NNW\n",
       "35063    NNE\n",
       "Name: wd, Length: 382168, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['wd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values in the **wd** column represent the wind speed. So, why not simply replace them with one-hot vectors like the **station** column?\n",
    "\n",
    "We could, but this obscures some information in the data. These are stored as categorical variables, but they correspond to angles. If we used one-hot vectors, we may lose that physical structure.\n",
    "\n",
    "Another option is to simply convert them to angles and store them in radians as $\\theta=0$, $\\frac{\\pi}{8}$, $\\frac{2\\pi}{8}$, ..., $\\frac{15\\pi}{8}$. (In reality, different angles are possible, but the original data was rouded to these values.) This leads to another problem: here, it will appear to the algorithm that, for example, the difference between an angles of $0$ and $\\frac{15\\pi}{8}$ will be much greater than the difference between angles of $0$ and $\\pi$, which is not quite right in this context.\n",
    "\n",
    "A better solution is to store the wind direction as a vector on the unit circle as $(\\cos(\\theta), \\sin(\\theta))$. So, let's write a simple function that converts radian measures to this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an angle to a list of unit circle coordinates\n",
    "def unitCircle(angle):\n",
    "    return [np.cos(angle), np.sin(angle)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's make a dictionary that maps each direction to the appropriate angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unitX</th>\n",
       "      <th>unitY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENE</th>\n",
       "      <td>9.238795e-01</td>\n",
       "      <td>3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNE</th>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNW</th>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NW</th>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WNW</th>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSW</th>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SW</th>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSW</th>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSE</th>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE</th>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESE</th>\n",
       "      <td>9.238795e-01</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            unitX         unitY\n",
       "E    1.000000e+00  0.000000e+00\n",
       "ENE  9.238795e-01  3.826834e-01\n",
       "NE   7.071068e-01  7.071068e-01\n",
       "NNE  3.826834e-01  9.238795e-01\n",
       "N    6.123234e-17  1.000000e+00\n",
       "NNW -3.826834e-01  9.238795e-01\n",
       "NW  -7.071068e-01  7.071068e-01\n",
       "WNW -9.238795e-01  3.826834e-01\n",
       "W   -1.000000e+00  1.224647e-16\n",
       "WSW -9.238795e-01 -3.826834e-01\n",
       "SW  -7.071068e-01 -7.071068e-01\n",
       "SSW -3.826834e-01 -9.238795e-01\n",
       "S   -1.836970e-16 -1.000000e+00\n",
       "SSE  3.826834e-01 -9.238795e-01\n",
       "SE   7.071068e-01 -7.071068e-01\n",
       "ESE  9.238795e-01 -3.826834e-01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all wind directions, along the unit circle\n",
    "directions = ['E', 'ENE', 'NE', 'NNE', 'N', 'NNW', 'NW', 'WNW', 'W', 'WSW', 'SW', 'SSW', 'S', 'SSE', 'SE', 'ESE']\n",
    "\n",
    "# make a dictionary associating each direction with coordinates on the unit circle \n",
    "directionDict = {direction : unitCircle(i*np.pi/8) for (i, direction) in enumerate(directions)}\n",
    "\n",
    "# create a dataframe from the dictionary\n",
    "directionDf = pd.DataFrame.from_dict(directionDict, orient = 'index', columns = ['unitX', 'unitY'])\n",
    "\n",
    "# display the dataframe\n",
    "directionDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to add **unitX** and **unitY** columns to our `data` dataframe, map the existing **wd** value to the appropriate $x$ and $y$ coordinates, and delete the **wd** column.\n",
    "\n",
    "`pandas` has the `join` function to take the dataframe we just created to do just this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>...</th>\n",
       "      <th>station_Guanyuan</th>\n",
       "      <th>station_Gucheng</th>\n",
       "      <th>station_Huairou</th>\n",
       "      <th>station_Nongzhanguan</th>\n",
       "      <th>station_Shunyi</th>\n",
       "      <th>station_Tiantan</th>\n",
       "      <th>station_Wanliu</th>\n",
       "      <th>station_Wanshouxigong</th>\n",
       "      <th>unitX</th>\n",
       "      <th>unitY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>0.382683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  ...  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  ...   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  ...   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  ...   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  ...   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  ...   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...  ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  ...   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  ...   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  ...   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  ...   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0  ...   \n",
       "\n",
       "       station_Guanyuan  station_Gucheng  station_Huairou  \\\n",
       "0                     0                0                0   \n",
       "1                     0                0                0   \n",
       "2                     0                0                0   \n",
       "3                     0                0                0   \n",
       "4                     0                0                0   \n",
       "...                 ...              ...              ...   \n",
       "35059                 0                0                0   \n",
       "35060                 0                0                0   \n",
       "35061                 0                0                0   \n",
       "35062                 0                0                0   \n",
       "35063                 0                0                0   \n",
       "\n",
       "       station_Nongzhanguan  station_Shunyi  station_Tiantan  station_Wanliu  \\\n",
       "0                         0               0                0               0   \n",
       "1                         0               0                0               0   \n",
       "2                         0               0                0               0   \n",
       "3                         0               0                0               0   \n",
       "4                         0               0                0               0   \n",
       "...                     ...             ...              ...             ...   \n",
       "35059                     0               0                0               0   \n",
       "35060                     0               0                0               0   \n",
       "35061                     0               0                0               0   \n",
       "35062                     0               0                0               0   \n",
       "35063                     0               0                0               0   \n",
       "\n",
       "       station_Wanshouxigong         unitX     unitY  \n",
       "0                          0 -3.826834e-01  0.923880  \n",
       "1                          0  6.123234e-17  1.000000  \n",
       "2                          0 -3.826834e-01  0.923880  \n",
       "3                          0 -7.071068e-01  0.707107  \n",
       "4                          0  6.123234e-17  1.000000  \n",
       "...                      ...           ...       ...  \n",
       "35059                      1 -7.071068e-01  0.707107  \n",
       "35060                      1 -9.238795e-01  0.382683  \n",
       "35061                      1 -7.071068e-01  0.707107  \n",
       "35062                      1 -3.826834e-01  0.923880  \n",
       "35063                      1  3.826834e-01  0.923880  \n",
       "\n",
       "[382168 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the direction dataframe with the data, mapping directions to unit circle coordinates\n",
    "data = data.join(directionDf, on = 'wd')\n",
    "\n",
    "# drop the wind direction column\n",
    "data = data.drop(columns = ['wd'])\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our data is totally numerical, so we can now fit a regression model!\n",
    "\n",
    "#### Train and Test Sets\n",
    "\n",
    "We convert the pollutant columns to a `NumPy` array by applying the `to_numpy()` function to `data`, selecting just the pollutant columns. These are our responses, or $y$ variables in the regression problem, `dataY`. We take the opposite columns to be the data matrix, `dataX`.\n",
    "\n",
    "Then, we use the `train_test_split` function to randomly assign 75\\% of the data to the training set and 25\\% to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutants = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "\n",
    "# pollutants are the responses\n",
    "dataY = data[pollutants].to_numpy()\n",
    "\n",
    "# all data except pollutants are predictors\n",
    "dataX = data.drop(columns = pollutants).to_numpy()\n",
    "\n",
    "# split the dataset and labels to 75% training set and 25% test set\n",
    "trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the dimensions of our newly created training and test data to see if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions\n",
      "(286626, 23)\n",
      "(286626, 6)\n",
      "\n",
      "Test set dimensions\n",
      "(95542, 23)\n",
      "(95542, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Training set dimensions')\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "print('\\nTest set dimensions')\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Separate Models for Each Response Variable\n",
    "\n",
    "Note that the training set will be quite large (286626 x 286626), which will be difficult with our simple matrix multiplication in the `OrdinaryLeastSquares` class we write, as it is likely to lead to overflows, so let's use the optimized `LinearRegression` class built into the popular `scikit-learn` library and predict each pollutant separately. If you are interested in linear algebra, it is good to know it uses singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Modeling PM2.5 pollution =====================\n",
      "The r^2 score is 0.23509684397917163\n",
      "The mean absolute error on the training set is 50.446976642553174\n",
      "The theta values are [-1.53000000e+00 -1.33000000e+00  0.00000000e+00  1.40000000e+00\n",
      " -6.03000000e+00 -1.38000000e+00  3.78000000e+00 -4.81000000e+00\n",
      " -2.94000000e+00  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  8.89000000e+00 -1.70700000e+01]\n",
      "The mean absolute error on the test set is 50.263222853725324\n",
      "\n",
      "==================== Modeling PM10 pollution =====================\n",
      "The r^2 score is 0.1608783939944075\n",
      "The mean absolute error on the training set is 60.592487618883155\n",
      "The theta values are [-2.82000000e+00 -1.23000000e+00  3.00000000e-01  1.71000000e+00\n",
      " -5.88000000e+00 -2.51000000e+00  2.82000000e+00 -5.94000000e+00\n",
      " -1.11000000e+00  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.82000000e+00 -1.99600000e+01]\n",
      "The mean absolute error on the test set is 60.5434715189066\n",
      "\n",
      "==================== Modeling SO2 pollution =====================\n",
      "The r^2 score is 0.26542270234422916\n",
      "The mean absolute error on the training set is 11.99849245468725\n",
      "The theta values are [-4.66000000e+00 -1.29000000e+00 -0.00000000e+00  2.30000000e-01\n",
      " -7.80000000e-01 -2.00000000e-01 -9.00000000e-02 -5.10000000e-01\n",
      " -1.76000000e+00  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  2.28000000e+00 -5.13000000e+00]\n",
      "The mean absolute error on the test set is 12.00910365209762\n",
      "\n",
      "==================== Modeling NO2 pollution =====================\n",
      "The r^2 score is 0.38335596352720613\n",
      "The mean absolute error on the training set is 21.003423156543647\n",
      "The theta values are [-1.63000000e+00  1.50000000e-01  6.00000000e-02  6.30000000e-01\n",
      " -2.28000000e+00 -6.90000000e-01  7.70000000e-01 -2.17000000e+00\n",
      " -7.41000000e+00  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.33000000e+00 -4.44000000e+00]\n",
      "The mean absolute error on the test set is 20.970252818211044\n",
      "\n",
      "==================== Modeling CO pollution =====================\n",
      "The r^2 score is 0.3025677327376025\n",
      "The mean absolute error on the training set is 657.5605630518952\n",
      "The theta values are [ 2.47000000e+00  1.02300000e+01 -2.30000000e+00  1.70400000e+01\n",
      " -9.59500000e+01 -2.25800000e+01  4.20100000e+01 -4.67500000e+01\n",
      " -8.64000000e+01  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  1.55130000e+02 -1.94050000e+02]\n",
      "The mean absolute error on the test set is 658.8088655545467\n",
      "\n",
      "==================== Modeling O3 pollution =====================\n",
      "The r^2 score is 0.5271211091704185\n",
      "The mean absolute error on the training set is 29.094786481283542\n",
      "The theta values are [ 1.60000000e+00 -1.80000000e+00 -2.00000000e-02  9.40000000e-01\n",
      "  3.93000000e+00  1.40000000e-01 -1.05000000e+00  1.02000000e+00\n",
      "  7.37000000e+00  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12 -1.72000000e+00 -1.22600000e+01]\n",
      "The mean absolute error on the test set is 29.162577292218337\n"
     ]
    }
   ],
   "source": [
    "# import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(trainY.shape[1]):\n",
    "    # choose the pollutant\n",
    "    print('\\n==================== Modeling', pollutants[i], 'pollution =====================')\n",
    "\n",
    "    # fit the model to the training data (find the beta parameters)\n",
    "    model.fit(trainX, trainY[:, i])\n",
    "\n",
    "    # return the predicted outputs for the datapoints in the training set\n",
    "    trainPredictions = model.predict(trainX)\n",
    "\n",
    "    # print the coefficient of determination r^2\n",
    "    print('The r^2 score is', model.score(trainX, trainY[:, i]))\n",
    "\n",
    "    # print quality metrics\n",
    "    print('The mean absolute error on the training set is', mean_absolute_error(trainY[:, i], trainPredictions))\n",
    "    \n",
    "    # print the beta values\n",
    "    print('The theta values are', np.round(model.coef_, 2))\n",
    "\n",
    "    # return the predicted outputs for the datapoints in the test set\n",
    "    predictions = model.predict(testX)\n",
    "\n",
    "    # print quality metrics\n",
    "    print('The mean absolute error on the test set is', mean_absolute_error(testY[:, i], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a Multivariate Regression Model\n",
    "\n",
    "The `OrdinaryLeastSquares` class we wrote was for multiple regression (multiple inputs) but not multivariate regression (multiple outputs). We will again use the `LinearRegression` class from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.31240712195924\n",
      "The mean absolute error on the training set is 138.44949471971347\n",
      "\n",
      "The theta values for predicting PM2.5 are\n",
      " [-1.53000000e+00 -1.33000000e+00  0.00000000e+00  1.40000000e+00\n",
      " -6.03000000e+00 -1.38000000e+00  3.78000000e+00 -4.81000000e+00\n",
      " -2.93000000e+00  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  6.64741189e+12  6.64741189e+12  6.64741189e+12\n",
      "  6.64741189e+12  8.89000000e+00 -1.70800000e+01]\n",
      "\n",
      "The theta values for predicting PM10 are\n",
      " [-2.82000000e+00 -1.23000000e+00  3.00000000e-01  1.71000000e+00\n",
      " -5.88000000e+00 -2.51000000e+00  2.82000000e+00 -5.95000000e+00\n",
      " -1.11000000e+00  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.57080851e+12  8.57080851e+12  8.57080851e+12\n",
      "  8.57080851e+12  8.82000000e+00 -1.99600000e+01]\n",
      "\n",
      "The theta values for predicting SO2 are\n",
      " [-4.66000000e+00 -1.29000000e+00 -0.00000000e+00  2.30000000e-01\n",
      " -7.80000000e-01 -2.00000000e-01 -9.00000000e-02 -5.10000000e-01\n",
      " -1.76000000e+00  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  4.32503155e+11  4.32503155e+11  4.32503155e+11\n",
      "  4.32503155e+11  2.28000000e+00 -5.13000000e+00]\n",
      "\n",
      "The theta values for predicting NO2 are\n",
      " [-1.63000000e+00  1.50000000e-01  6.00000000e-02  6.30000000e-01\n",
      " -2.28000000e+00 -6.90000000e-01  7.70000000e-01 -2.17000000e+00\n",
      " -7.41000000e+00  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.69990342e+11  3.69990342e+11  3.69990342e+11\n",
      "  3.69990342e+11  3.33000000e+00 -4.44000000e+00]\n",
      "\n",
      "The theta values for predicting CO are\n",
      " [ 2.47000000e+00  1.02300000e+01 -2.30000000e+00  1.70400000e+01\n",
      " -9.59500000e+01 -2.25800000e+01  4.20100000e+01 -4.67500000e+01\n",
      " -8.64000000e+01  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  6.24061319e+13  6.24061319e+13  6.24061319e+13\n",
      "  6.24061319e+13  1.55130000e+02 -1.94050000e+02]\n",
      "\n",
      "The theta values for predicting O3 are\n",
      " [ 1.60000000e+00 -1.80000000e+00 -2.00000000e-02  9.40000000e-01\n",
      "  3.93000000e+00  1.40000000e-01 -1.05000000e+00  1.02000000e+00\n",
      "  7.37000000e+00  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12  5.56156658e+12  5.56156658e+12  5.56156658e+12\n",
      "  5.56156658e+12 -1.72000000e+00 -1.22600000e+01]\n",
      "\n",
      "The mean absolute error on the test set is 138.62629313747678\n"
     ]
    }
   ],
   "source": [
    "# instantiate an OLS model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', model.score(trainX, trainY))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the theta values\n",
    "thetas = [np.round(row, 2) for row in model.coef_]\n",
    "for i in range(6):\n",
    "    print('\\nThe theta values for predicting', pollutants[i], 'are\\n', thetas[i])\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nThe mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might note, there is actually no difference between the models in the last two code blocks. The parameters are the same for each pollutant. The only difference is that we have wholistic measures of quality, which are simply averages of the results from above.\n",
    "\n",
    "In a sense, the linear regression models are just stacked on one another. It turns out, this is a common situation in machine learning. In fact, neural networks (deep learning) that solve regression problems can be considered as stacked models. The difference is that neural networks allows the responses variable to *share* parameters--so all response variables would be influenced by all of the parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Notation\n",
    "\n",
    "There is an unfortunate situation common to multidisciplinary fields like machine learning: the norms of notation vary in different subfields and related disciplines.\n",
    "\n",
    "In particular, the notation for model parameters in machine learning unfortunately depends on the subfield where you are reading--$\\beta$ is common in statistical literature on linear regression, $w$ or $\\alpha$ and $b$ are common in neural networks and deep learning literature, and $\\theta$ is common in numerical mathematics and the wider machine learning literature. As a general rule, be careful to read when the notation is introduced in books or papers. The context may not always make it clear.\n",
    "\n",
    "Going forward, we will follow the norms of *The Elements of Statistical Learning* by Hastie, et. al., and others and refer to model parameters as $\\theta_i$.\n",
    "\n",
    "### Limitations of Fitting Lines, Planes, and Hyperplanes\n",
    "\n",
    "So far, we have used linear regression to find the best-fit line ($d=1$), plane ($d=2$), or hyperplane ($d\\geq 3$) by ordinary least squares. Mathematically, this means we assumed our regression function was in the form\n",
    "\n",
    "$$\\hat{f}(x_i)=\\theta^Tx_i$$\n",
    "\n",
    "We fit the \"best\" line/plane/hyperplane by constructing the least squares loss function\n",
    "\n",
    "$$L(\\theta)=\\sum\\limits_{i=1}^n (f(x_i) - y_i)^2=\\|X\\theta - y\\|^2_2$$\n",
    "\n",
    "and solving the minimization problem\n",
    "\n",
    "$$\\min\\limits_{\\theta} L(\\theta)$$\n",
    "\n",
    "which gave us a simple formula for the coefficients,\n",
    "\n",
    "$$\\theta = (X^TX)^{-1}Xy$$\n",
    "\n",
    "that we implemented and used on a few examples.\n",
    "\n",
    "In some cases, fitting hyperplanes gave some very good results. However, sometimes these shapes just do not fit well. Examples:\n",
    "\n",
    "* Predicting hourly temperatures in Melbourne, FL over a span of several days with a line would be a bad idea. It should oscillate as temperatures go down at night and back up in the daytime.\n",
    "\n",
    "* Predicting the number of COVID cases in the US per day or the value of an investment over time with an interest rate with a line would not work well because we know these things grow exponentially.\n",
    "\n",
    "* The best-fit pollution model from the example above did not explain most of the variation in the pollution data for most pollutants.\n",
    "\n",
    "Here, fitting lines or hyperplanes does not work, so there are a number of other least squares linear models for regression that attempt to solve these types of problems that we will study over the next couple of weeks. In general, we need to solve the optimization problem\n",
    "\n",
    "$$\\min\\limits_{f,\\theta} L(f,\\theta)$$\n",
    "\n",
    "for some loss function $L$ where $\\theta$ can once again be any real vector of parameters, but there is a new part: $f$ can have a different structure. Now, without further restrictions, this problem has infinitely many solutions.\n",
    "\n",
    "Indeed, if you consider $d=1$, then the dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with distinct $x_i$'s is just some points on the 2D plane, so there are infinitely many functions $f$ that will give $L = 0$ when using the sum of squares loss function.\n",
    "\n",
    "**Math Note**: Not only are there infinitely many solutions, but the space of functions with 0 loss is actually infinite-dimensional!\n",
    "\n",
    "We have two main problems here:\n",
    "\n",
    "1. Sometimes there is no line/plane/hyperplane that maps the inputs to the outputs well\n",
    "2. The optimization problem has infinitely many solutions when we do not assume $f$ has a particular structure\n",
    "\n",
    "When we have the first problem, clearly we need to use a different type of function $f$ if we hope to build a good model. With the second problem, there are broadly three classes of remedies in linear regression:\n",
    "\n",
    "* **Basis function methods** restrict $f$ to be within a pre-defined set of functions linear in the $\\theta_i$'s but apply some nonlinear function(s) to the inputs.\n",
    "\n",
    "* **Kernel regression methods** adjust the loss function $L$ by weighting the sum of errors by localized properties like proximity to nearby training points.\n",
    "\n",
    "* **Regularization methods** adjust the loss function $L$ by adding terms to the loss function that *penalize* negative characteristics of fitted functions\n",
    "\n",
    "Many approaches combine more than 1 of these ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
