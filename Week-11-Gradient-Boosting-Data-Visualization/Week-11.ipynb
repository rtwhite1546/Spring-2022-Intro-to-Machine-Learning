{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d305a4-dcb8-48e9-9f92-ea6e87145e26",
   "metadata": {},
   "source": [
    "# Week 11 - Gradient Boosting and Optimization-based Data Visualization\n",
    "\n",
    "Last week, we considered boosting and AdaBoost, a powerful ensemble method for classification or regression.\n",
    "\n",
    "This week, we will learn about another variation of boosting: **gradient boosting** and the **XGBoost** algorithm, currently one of the best classifiers for many problems.\n",
    "\n",
    "This completes the supervised learning portion of the course, and we will focus on **unsupervised learning** starting Thursday.\n",
    "\n",
    "In unsupervised learning, we attempt to perform some tasks on a dataset $x_1$, ..., $x_n$ *without* labels. Common tasks include data visualization, dimensionality reduction, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b69b5b-344b-4cbf-b644-d1bdac2546f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 20 - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0089ca6f-251c-469b-8102-954bd6c25408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541f976-ff6d-4b9b-874a-1457ab3b9a04",
   "metadata": {},
   "source": [
    "Let's try gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a332135-82aa-41f0-90f1-a1c9e33bf0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2])\n",
    "trainX = trainX.astype('float')/255.0\n",
    "\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1] * testX.shape[2])\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "# build the classifier\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709da86-118c-4940-88f7-cb9ca677e8f4",
   "metadata": {},
   "source": [
    "Let's also try the `HistGradientBoostingClassifier`, an experimental implementation of an algorithm similar to ordinary `GradientBoostingClassifier` but it sometimes runs much faster on large datasets. It is a `scikit-learn` implementation based on Microsoft's [Light GBM](https://github.com/Microsoft/LightGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a95c0e-f7a3-4c5f-a75b-50ec7f87e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2])\n",
    "trainX = trainX.astype('float')/255.0\n",
    "\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1] * testX.shape[2])\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "# build the classifier\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fc95d-a3df-4a15-a334-a0f15bfb6cff",
   "metadata": {},
   "source": [
    "### CIFAR-10 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92c118-e6f5-4a0b-9794-048a17525ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2] * 3)\n",
    "trainX = trainX.astype('float')/255.0\n",
    "\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1] * testX.shape[2] * 3)\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "# build the classifier\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f62b2-5515-46e0-bbe5-21cb66294f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e8b1a-bc68-4c87-8d7e-494809178b01",
   "metadata": {},
   "source": [
    "### Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea136308-bfad-4e19-9cff-5aade22fd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X = cancer['data']\n",
    "Y = cancer['target']\n",
    "\n",
    "# split the data into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "# build the classifier\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metricsl\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cccb0a-6aed-4e03-8cbb-3cf508e6ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(model, testX, testY, n_repeats = 10, n_jobs = -1)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T, vert = False,\n",
    "           labels = cancer.feature_names[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b0400-2403-4e70-997e-8d18647f6469",
   "metadata": {},
   "source": [
    "### Wine Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdae9a4-007e-4665-bcc7-6febe871f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "\n",
    "X = wine['data']\n",
    "Y = wine['target']\n",
    "\n",
    "# split the data into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "# build the classifier\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d065b-0e45-4316-82c4-79443c744d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(model, testX, testY, n_repeats = 10, n_jobs = -1)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T, vert = False,\n",
    "           labels = np.array(wine.feature_names)[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46ec2a-9f98-4c35-9ddd-0d2801f25f34",
   "metadata": {},
   "source": [
    "### MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64765edf-9d1e-456f-95c9-50642b589098",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2])\n",
    "trainX = trainX.astype('float')/255.0\n",
    "\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1] * testX.shape[2])\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "# build the classifier\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506b85c-293d-4415-bc73-ecd1f9689224",
   "metadata": {},
   "source": [
    "Let's try to tune XGBoost with 5-fold cross-validation to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d966c36-15f6-411b-b4ea-50d4f63d7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [1, 5, 10, 100, None], 'reg_lambda': [0.001, 0.01, 0.1, 1, 10]}\n",
    "model = GridSearchCV(XGBClassifier(n_jobs = -1, eval_metric='mlogloss'), parameters, n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ffa14-014c-4180-9f0c-14e4ac859244",
   "metadata": {},
   "source": [
    "So, we tried, but made no gains.\n",
    "\n",
    "### CIFAR10 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9f1c1-3f12-4177-a7cf-6d857c79b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2] * 3)\n",
    "trainX = trainX.astype('float')/255.0\n",
    "\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1] * testX.shape[2] * 3)\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "# build the classifier\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a5515-d095-4e7a-803a-0cd527223c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [1, 5, 10, 100, None], 'reg_lambda': [0.001, 0.01, 0.1, 1, 10]}\n",
    "model = GridSearchCV(XGBClassifier(n_jobs = -1, eval_metric='mlogloss'), parameters, n_jobs = -1)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# predict the labels of the training set\n",
    "predictedY = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTraining Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nTesting Classification Report:\\n\\n', classification_report(testY, predictedY))\n",
    "\n",
    "# predict the labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "print('\\nTesting Confusion Matrix:\\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78af45-a301-4b9c-9bf7-d51951527820",
   "metadata": {},
   "source": [
    "# Lecture 21 - Optimization-based Data Visualization\n",
    "\n",
    "*Please see the lecture video and notes in Canvas.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
